# The Impact of Missing Data on the Hausman Test: A Monte Carlo Study
# Author: Eungi Kim
# Version: 1.0.0, Date: July 2025, License: MIT
# Description: Monte Carlo simulation to evaluate missing data effects on Hausman test performance
# Requirements: R ≥4.0.0, packages: plm, dplyr, tidyr, purrr, ggplot2, scales, knitr, broom
# Usage: Set working directory, run: source("hausman_monte_carlo.R"), results saved to ./results/

# 1. ENVIRONMENT SETUP AND DEPENDENCIES
rm(list = ls())
gc()

required_packages <- c("plm", "dplyr", "tidyr", "purrr", "ggplot2", "scales", "knitr", "broom")
for (pkg in required_packages) {
  if (!require(pkg, character.only = TRUE)) {
    install.packages(pkg)
    library(pkg, character.only = TRUE)
  }
}
library(dplyr, warn.conflicts = FALSE)
library(tidyr, warn.conflicts = FALSE)
library(purrr, warn.conflicts = FALSE)
library(ggplot2, warn.conflicts = FALSE)

# 2. SIMULATION CONFIGURATION
set.seed(1978)
N_REPLICATIONS <- 100
ALPHA_LEVEL <- 0.05
MIN_VIABLE_MECHANISM_N <- 50
MIN_SUCCESSFUL_RUNS <- 20

panel_architectures <- list(
  "Wide Panel" = list(N = 400, T = 4),
  "Square" = list(N = 200, T = 8),
  "Long Panel" = list(N = 100, T = 16)
)

variable_complexities <- list(
  "Simple" = 1,
  "Standard" = 3,
  "Complex" = 6,
  "High-Dimensional" = 10
)

missingness_mechanisms <- c("Random", "Early Exit", "Late Missing", "Cyclical")
dropout_rates <- c(0.10, 0.20, 0.30, 0.40)

# 3. DATA GENERATION AND MISSING DATA FUNCTIONS
generate_panel_data <- function(N, T, k) {
  panel_ids <- expand.grid(id = 1:N, time = 1:T)
  alpha <- rnorm(N, mean = 0, sd = 2)
  panel_ids$alpha <- alpha[panel_ids$id]
  x_vars <- matrix(rnorm(N * T * k), nrow = N * T, ncol = k)
  colnames(x_vars) <- paste0("x", 1:k)
  epsilon <- rnorm(N * T, mean = 0, sd = 1.5)
  df <- data.frame(panel_ids, x_vars)
  df$y <- rowSums(df[, paste0("x", 1:k), drop = FALSE]) + df$alpha + epsilon
  plm::pdata.frame(df, index = c("id", "time"))
}

introduce_random_missing <- function(df, delta) {
  n_obs <- nrow(df)
  missing_indices <- sample(1:n_obs, size = round(delta * n_obs))
  x_cols <- grep("^x", names(df), value = TRUE)
  df[missing_indices, c("y", x_cols)] <- NA
  df
}

introduce_early_exit <- function(df, delta) {
  N <- length(unique(df$id))
  T_total <- length(unique(df$time))
  x_cols <- grep("^x", names(df), value = TRUE)
  units_to_drop_count <- max(1, round(2 * N * delta))
  if (delta == 0 || units_to_drop_count == 0) return(df)
  units_to_drop_ids <- sample(unique(df$id), size = units_to_drop_count)
  for (unit_id in units_to_drop_ids) {
    T_exit <- sample(2:T_total, 1)
    df[df$id == unit_id & df$time >= T_exit, c("y", x_cols)] <- NA
  }
  df
}

introduce_late_missing <- function(df, delta) {
  T_total <- length(unique(df$time))
  x_cols <- grep("^x", names(df), value = TRUE)
  c_val <- 2 * delta / (T_total + 1)
  df <- df %>%
    group_by(id) %>%
    mutate(
      prob_missing = pmin(c_val * time, 0.95),
      is_missing = runif(n()) < prob_missing
    ) %>%
    ungroup()
  df[df$is_missing, c("y", x_cols)] <- NA
  df$prob_missing <- NULL
  df$is_missing <- NULL
  df
}

introduce_cyclical_missing <- function(df, delta) {
  T_total <- length(unique(df$time))
  x_cols <- grep("^x", names(df), value = TRUE)
  high_periods <- floor(T_total/2):ceiling(T_total/2 + 1)
  T_high <- length(high_periods)
  T_low <- T_total - T_high
  if ((3*T_high + T_low) == 0) return(df)
  p_low <- (T_total * delta) / (3*T_high + T_low)
  p_high <- min(3 * p_low, 0.95)
  df$prob <- ifelse(df$time %in% high_periods, p_high, p_low)
  df$is_missing <- runif(nrow(df)) < df$prob
  df[df$is_missing, c("y", x_cols)] <- NA
  df$prob <- NULL
  df$is_missing <- NULL
  df
}

# 4. SIMULATION EXECUTION FUNCTION
run_single_simulation <- function(params) {
  tryCatch({
    complete_data <- generate_panel_data(params$N, params$T, params$k)
    missing_data <- if (params$delta > 0) {
      switch(params$mechanism,
             "Random" = introduce_random_missing(complete_data, params$delta),
             "Early Exit" = introduce_early_exit(complete_data, params$delta),
             "Late Missing" = introduce_late_missing(complete_data, params$delta),
             "Cyclical" = introduce_cyclical_missing(complete_data, params$delta))
    } else {
      complete_data
    }
    obs_per_id <- missing_data %>% 
      filter(!is.na(y)) %>%
      group_by(id) %>%
      summarise(n_obs = n(), .groups = 'drop')
    valid_ids <- obs_per_id %>% filter(n_obs > max(1, params$k)) %>% pull(id)
    if (length(valid_ids) < 2) {
      return(list(p_value = NA, failure_reason = "Data Failure: Insufficient individuals"))
    }
    final_data <- missing_data %>% 
      filter(id %in% valid_ids) %>% 
      plm::pdata.frame(index = c("id", "time"))
    formula_str <- paste("y ~", paste(paste0("x", 1:params$k), collapse = " + "))
    formula <- as.formula(formula_str)
    fe_model <- tryCatch({ plm::plm(formula, data = final_data, model = "within") }, 
                         error = function(e) NULL)
    if (is.null(fe_model)) {
      return(list(p_value = NA, failure_reason = "Model Failure: FE model failed"))
    }
    re_model <- tryCatch({ plm::plm(formula, data = final_data, model = "random") },
                         error = function(e) NULL)
    if (is.null(re_model)) {
      return(list(p_value = NA, failure_reason = "Model Failure: RE model failed"))
    }
    hausman_test_result <- tryCatch({ plm::phtest(fe_model, re_model) },
                                   error = function(e) NULL)
    if (is.null(hausman_test_result)) {
      return(list(p_value = NA, failure_reason = "Hausman Test Failure"))
    }
    list(p_value = hausman_test_result$p.value, failure_reason = "Success")
  }, error = function(e) {
    list(p_value = NA, failure_reason = "System Error")
  })
}

# 5. PRE-FILTERING
all_scenarios <- expand.grid(
  panel_name = names(panel_architectures),
  complexity_name = names(variable_complexities),
  mechanism = missingness_mechanisms,
  delta = dropout_rates,
  stringsAsFactors = FALSE
) %>%
  mutate(
    N = map_int(panel_name, ~panel_architectures[[.]]$N),
    T = map_int(panel_name, ~panel_architectures[[.]]$T),
    k = map_int(complexity_name, ~variable_complexities[[.]])
  )

impossible_scenarios <- all_scenarios %>%
  filter(T < k + 1) %>%
  mutate(exclusion_reason = "Mathematical: T < k+1")

feasible_scenarios <- all_scenarios %>%
  filter(T >= k + 1)

# 6. RUN MONTE CARLO SIMULATION
all_results <- list()
start_time <- Sys.time()
timestamp <- format(Sys.time(), "%Y%m%d_%H%M%S")

for (i in 1:nrow(feasible_scenarios)) {
  params <- as.list(feasible_scenarios[i, ])
  scenario_results <- map(1:N_REPLICATIONS, ~ run_single_simulation(params))
  all_results[[i]] <- bind_rows(scenario_results) %>%
    mutate(
      panel_name = params$panel_name,
      complexity_name = params$complexity_name,
      mechanism = params$mechanism,
      delta = params$delta,
      N = params$N,
      T = params$T,
      k = params$k,
      specificity = as.numeric(p_value > ALPHA_LEVEL)
    )
}

final_results_df <- bind_rows(all_results)
results_dir <- "results"
if (!dir.exists(results_dir)) dir.create(results_dir, recursive = TRUE)
output_filename <- sprintf("simulation_results_%d_runs_%s.csv", nrow(final_results_df), timestamp)
write.csv(final_results_df, file.path(results_dir, output_filename), row.names = FALSE)
write.csv(impossible_scenarios, file.path(results_dir, paste0("excluded_impossible_scenarios_", timestamp, ".csv")), row.names = FALSE)

# 7. FAILURE ANALYSIS
failure_by_mechanism <- final_results_df %>%
  group_by(mechanism) %>%
  summarise(
    total_runs = n(),
    successful_runs = sum(failure_reason == "Success", na.rm = TRUE),
    failure_rate = 1 - (successful_runs / total_runs),
    .groups = 'drop'
  ) %>%
  arrange(desc(failure_rate))
write.csv(failure_by_mechanism, file.path(results_dir, paste0("failure_analysis_by_mechanism_", timestamp, ".csv")), row.names = FALSE)

# 8. PREPARE DATA FOR TABLES
test_data <- final_results_df %>%
  filter(!is.na(p_value), failure_reason == "Success") %>%
  mutate(
    specificity = as.numeric(p_value > ALPHA_LEVEL),
    panel_name = factor(panel_name, levels = c("Wide Panel", "Square", "Long Panel")),
    complexity_name = factor(complexity_name, levels = c("Simple", "Standard", "Complex", "High-Dimensional")),
    mechanism = factor(mechanism, levels = c("Random", "Early Exit", "Late Missing", "Cyclical")),
    delta_factor = factor(delta)
  )

mechanism_counts <- test_data %>%
  count(mechanism) %>%
  arrange(desc(n))
viable_mechanisms <- mechanism_counts %>%
  filter(n >= MIN_VIABLE_MECHANISM_N) %>%
  pull(mechanism)

# 9. GENERATE TABLES
# TABLE 1: Statistical Comparison of Mechanisms
if (length(viable_mechanisms) == 2) {
  mech1 <- viable_mechanisms[1]
  mech2 <- viable_mechanisms[2]
  table1 <- test_data %>%
    filter(mechanism %in% c(mech1, mech2)) %>%
    group_by(delta, mechanism) %>%
    summarise(
      mean_specificity = mean(specificity, na.rm = TRUE),
      n = n(),
      .groups = 'drop'
    ) %>%
    pivot_wider(names_from = mechanism, values_from = c(mean_specificity, n)) %>%
    rowwise() %>%
    mutate(
      dropout_rate = sprintf("%.0f%%", delta * 100),
      mech1_col = sprintf("%.3f (%d)", get(paste0("mean_specificity_", mech1)), get(paste0("n_", mech1))),
      mech2_col = sprintf("%.3f (%d)", get(paste0("mean_specificity_", mech2)), get(paste0("n_", mech2))),
      difference = get(paste0("mean_specificity_", mech1)) - get(paste0("mean_specificity_", mech2)),
      t_stat = {
        x <- test_data$specificity[test_data$mechanism == mech1 & test_data$delta == delta]
        y <- test_data$specificity[test_data$mechanism == mech2 & test_data$delta == delta]
        if(length(x) > 5 && length(y) > 5) t.test(x, y)$statistic else NA
      },
      p_value = {
        x <- test_data$specificity[test_data$mechanism == mech1 & test_data$delta == delta]
        y <- test_data$specificity[test_data$mechanism == mech2 & test_data$delta == delta]
        if(length(x) > 5 && length(y) > 5) t.test(x, y)$p.value else NA
      },
      significance = ifelse(is.na(p_value), "N/A", ifelse(p_value < 0.05, "Yes", "No"))
    ) %>%
    ungroup() %>%
    select(dropout_rate, mech1_col, mech2_col, difference, t_stat, p_value, significance)
  names(table1)[2] <- paste(mech1, "(n)")
  names(table1)[3] <- paste(mech2, "(n)")
  write.csv(table1, file.path(results_dir, paste0("table1_statistical_comparison_", timestamp, ".csv")), row.names = FALSE)
}

# TABLE 2: Overall Failure Rates by Mechanism
table2 <- failure_by_mechanism %>%
  mutate(
    failed_runs = total_runs - successful_runs,
    failure_rate_percent = sprintf("%.1f%%", failure_rate * 100),
    success_rate_percent = sprintf("%.1f%%", (1 - failure_rate) * 100)
  ) %>%
  select(
    Mechanism = mechanism,
    Total_Runs = total_runs,
    Successful_Runs = successful_runs,
    Failed_Runs = failed_runs,
    Failure_Rate = failure_rate_percent,
    Success_Rate = success_rate_percent
  ) %>%
  arrange(desc(Failed_Runs))
write.csv(table2, file.path(results_dir, paste0("table2_overall_failure_rates_", timestamp, ".csv")), row.names = FALSE)

# TABLE 3: Failure Analysis for High-Complexity Scenarios
available_long_highdim <- final_results_df %>%
  filter(panel_name == "Long Panel", complexity_name == "High-Dimensional") %>%
  nrow()
if (available_long_highdim > 0) {
  table3 <- final_results_df %>%
    filter(panel_name == "Long Panel", complexity_name == "High-Dimensional") %>%
    group_by(mechanism, delta, failure_reason) %>%
    summarise(count = n(), .groups = 'drop') %>%
    mutate(dropout_rate = sprintf("%.0f%%", delta * 100)) %>%
    select(Mechanism = mechanism, Dropout_Rate = dropout_rate, Failure_Reason = failure_reason, Count = count) %>%
    arrange(Mechanism, Dropout_Rate, desc(Count))
  write.csv(table3, file.path(results_dir, paste0("table3_failure_analysis_long_highdim_", timestamp, ".csv")), row.names = FALSE)
}

# TABLE 4: Specificity by Panel Architecture and Model Complexity
run_stats <- final_results_df %>%
  group_by(panel_name, complexity_name) %>%
  summarise(
    total_runs = n(),
    successful_runs = sum(failure_reason == "Success", na.rm = TRUE),
    avg_specificity = ifelse(successful_runs >= MIN_SUCCESSFUL_RUNS, 
                             mean(specificity[failure_reason == "Success"], na.rm = TRUE), 
                             NA),
    .groups = 'drop'
  )
impossible_combos <- impossible_scenarios %>%
  select(panel_name, complexity_name) %>%
  distinct() %>%
  mutate(total_runs = 0, successful_runs = 0, avg_specificity = NA)
all_stats <- bind_rows(run_stats, impossible_combos)
get_cell_value <- function(panel, complexity, stats_df) {
  row <- stats_df[stats_df$panel_name == panel & stats_df$complexity_name == complexity, ]
  if (any(impossible_scenarios$panel_name == panel & impossible_scenarios$complexity_name == complexity)) {
    return("Impossible")
  }
  if (nrow(row) == 0) return("Unknown")
  if (!is.na(row$total_runs) && row$total_runs > 0 && row$successful_runs < MIN_SUCCESSFUL_RUNS) {
    return("Failed")
  }
  if (!is.na(row$avg_specificity) && row$successful_runs >= MIN_SUCCESSFUL_RUNS) {
    return(as.character(round(row$avg_specificity, 4)))
  }
  return("Unknown")
}
panel_obs <- sapply(names(panel_architectures), function(name) {
  arch <- panel_architectures[[name]]
  arch$N * arch$T
})
all_attempts_table <- data.frame(
  panel_name = rep(c("Wide Panel", "Square", "Long Panel"), each = 1),
  panel_label = paste0(c("Wide Panel", "Square", "Long Panel"), " (All attempts)"),
  N = panel_obs,
  Simple = sapply(c("Wide Panel", "Square", "Long Panel"), 
                  function(p) get_cell_value(p, "Simple", all_stats)),
  Standard = sapply(c("Wide Panel", "Square", "Long Panel"), 
                    function(p) get_cell_value(p, "Standard", all_stats)),
  Complex = sapply(c("Wide Panel", "Square", "Long Panel"), 
                   function(p) get_cell_value(p, "Complex", all_stats)),
  `High-Dimensional` = sapply(c("Wide Panel", "Square", "Long Panel"), 
                             function(p) get_cell_value(p, "High-Dimensional", all_stats)),
  stringsAsFactors = FALSE,
  check.names = FALSE
)
all_attempts_table$Average <- sapply(1:nrow(all_attempts_table), function(i) {
  values <- c(all_attempts_table$Simple[i], all_attempts_table$Standard[i], 
              all_attempts_table$Complex[i], all_attempts_table$`High-Dimensional`[i])
  numeric_values <- suppressWarnings(as.numeric(values[!values %in% c("Impossible", "Failed", "Unknown")]))
  if (length(numeric_values) == 0) return("—")
  round(mean(numeric_values, na.rm = TRUE), 4)
})
successful_only_rows <- data.frame()
square_complex <- all_stats[all_stats$panel_name == "Square" & all_stats$complexity_name == "Complex", ]
if (nrow(square_complex) > 0 && square_complex$successful_runs >= MIN_SUCCESSFUL_RUNS && square_complex$successful_runs < panel_obs["Square"]) {
  successful_only_rows <- rbind(successful_only_rows, data.frame(
    panel_name = "Square",
    panel_label = "Square (Successful only)",
    N = square_complex$successful_runs,
    Simple = "—",
    Standard = "—",
    Complex = as.character(round(square_complex$avg_specificity, 4)),
    `High-Dimensional` = "Impossible",
    Average = as.character(round(square_complex$avg_specificity, 4)),
    stringsAsFactors = FALSE,
    check.names = FALSE
  ))
}
long_highdim <- all_stats[all_stats$panel_name == "Long Panel" & all_stats$complexity_name == "High-Dimensional", ]
if (nrow(long_highdim) > 0 && long_highdim$successful_runs >= MIN_SUCCESSFUL_RUNS && long_highdim$successful_runs < panel_obs["Long Panel"]) {
  successful_only_rows <- rbind(successful_only_rows, data.frame(
    panel_name = "Long Panel",
    panel_label = "Long Panel (Successful only)",
    N = long_highdim$successful_runs,
    Simple = "—",
    Standard = "—",
    Complex = "—",
    `High-Dimensional` = as.character(round(long_highdim$avg_specificity, 4)),
    Average = as.character(round(long_highdim$avg_specificity, 4)),
    stringsAsFactors = FALSE,
    check.names = FALSE
  ))
}
table4_final <- rbind(all_attempts_table, successful_only_rows) %>%
  arrange(
    match(panel_name, c("Wide Panel", "Square", "Long Panel")),
    desc(grepl("All attempts", panel_label))
  ) %>%
  select(`Panel Name (Sample Type)` = panel_label, N, Simple, Standard, Complex, `High-Dimensional`, Average)
write.csv(table4_final, file.path(results_dir, paste0("table4_specificity_by_architecture_complexity_", timestamp, ".csv")), row.names = FALSE)

# TABLE 5: ANOVA Results
if (nrow(test_data) > 100) {
  group_counts <- test_data %>%
    group_by(panel_name, complexity_name) %>%
    summarise(n = n(), .groups = 'drop') %>%
    filter(n < 2)
  if (nrow(group_counts) == 0) {
    panel_anova <- aov(specificity ~ panel_name, data = test_data)
    panel_summary <- summary(panel_anova)[[1]]
    complexity_anova <- aov(specificity ~ complexity_name, data = test_data)
    complexity_summary <- summary(complexity_anova)[[1]]
    interaction_anova <- aov(specificity ~ panel_name * complexity_name, data = test_data)
    interaction_summary <- summary(interaction_anova)[[1]]
    table5 <- data.frame(
      Effect = c("Panel Architecture", "Model Complexity", "Panel Architecture × Model Complexity"),
      df = c(panel_summary$Df[1], complexity_summary$Df[1], 
             ifelse(nrow(interaction_summary) >= 3, interaction_summary$Df[3], NA)),
      F_statistic = c(panel_summary$`F value`[1], complexity_summary$`F value`[1],
                     ifelse(nrow(interaction_summary) >= 3, interaction_summary$`F value`[3], NA)),
      p_value = c(panel_summary$`Pr(>F)`[1], complexity_summary$`Pr(>F)`[1],
                 ifelse(nrow(interaction_summary) >= 3, interaction_summary$`Pr(>F)`[3], NA)),
      Significance = sapply(c(panel_summary$`Pr(>F)`[1], complexity_summary$`Pr(>F)`[1],
                             ifelse(nrow(interaction_summary) >= 3, interaction_summary$`Pr(>F)`[3], NA)), 
                           function(p) {
                             if(is.na(p)) return("N/A")
                             ifelse(p < 0.001, "***", ifelse(p < 0.05, "*", "ns"))
                           })
    )
    write.csv(table5, file.path(results_dir, paste0("table5_anova_results_", timestamp, ".csv")), row.names = FALSE)
  }
}

# TABLE 6: Main Effects of Panel Architecture
table6 <- test_data %>%
  group_by(panel_name) %>%
  summarise(
    mean_specificity = mean(specificity, na.rm = TRUE),
    sd_specificity = sd(specificity, na.rm = TRUE),
    n = n(),
    se = sd_specificity / sqrt(n),
    ci_lower = mean_specificity - 1.96 * se,
    ci_upper = mean_specificity + 1.96 * se,
    .groups = 'drop'
  ) %>%
  mutate_if(is.numeric, ~round(.x, 4))
write.csv(table6, file.path(results_dir, paste0("table6_panel_architecture_effects_", timestamp, ".csv")), row.names = FALSE)

# TABLE 7: Comprehensive ANOVA
if (nrow(test_data) > 100 && length(viable_mechanisms) > 1) {
  full_anova <- aov(specificity ~ mechanism + panel_name + complexity_name + panel_name:complexity_name, 
                   data = test_data)
  full_summary <- summary(full_anova)[[1]]
  effect_names <- rownames(full_summary)[1:(nrow(full_summary)-1)]
  effect_names[effect_names == "panel_name:complexity_name"] <- "Panel Architecture × Model Complexity"
  table7 <- data.frame(
    Effect = effect_names,
    df = full_summary$Df[1:(nrow(full_summary)-1)],
    F_statistic = round(full_summary$`F value`[1:(nrow(full_summary)-1)], 4),
    p_value = round(full_summary$`Pr(>F)`[1:(nrow(full_summary)-1)], 4),
    Significance = sapply(full_summary$`Pr(>F)`[1:(nrow(full_summary)-1)], function(p) {
      if(is.na(p)) return("N/A")
      ifelse(p < 0.001, "***", ifelse(p < 0.05, "*", "ns"))
    })
  )
  write.csv(table7, file.path(results_dir, paste0("table7_comprehensive_anova_", timestamp, ".csv")), row.names = FALSE)
}

# TABLE 8: Pairwise Comparisons
if (nrow(test_data) > 100) {
  panel_levels <- levels(test_data$panel_name)
  panel_comparisons <- list()
  for (i in 1:(length(panel_levels)-1)) {
    for (j in (i+1):length(panel_levels)) {
      level1 <- panel_levels[i]
      level2 <- panel_levels[j]
      data1 <- test_data$specificity[test_data$panel_name == level1]
      data2 <- test_data$specificity[test_data$panel_name == level2]
      if (length(data1) >= 10 && length(data2) >= 10 && var(data1) > 0 && var(data2) > 0) {
        test_result <- t.test(data1, data2)
        panel_comparisons[[paste(level1, "vs", level2)]] <- data.frame(
          Factor = "Panel Architecture",
          Comparison = paste(level1, "vs", level2),
          p_value = round(test_result$p.value, 4),
          Significance = ifelse(test_result$p.value < 0.001, "***", 
                               ifelse(test_result$p.value < 0.05, "*", "ns")),
          Interpretation = ifelse(test_result$p.value < 0.05, 
                                 ifelse(mean(data1) > mean(data2), paste(level1, "better"), paste(level2, "better")),
                                 "No difference")
        )
      }
    }
  }
  if (length(panel_comparisons) > 0) {
    table8 <- bind_rows(panel_comparisons)
    write.csv(table8, file.path(results_dir, paste0("table8_pairwise_comparisons_", timestamp, ".csv")), row.names = FALSE)
  }
}

# 10. GENERATE FIGURES
# FIGURE 1: Failure Rates Heatmap
failure_rates_complete <- final_results_df %>%
  group_by(panel_name, complexity_name) %>%
  summarise(
    failure_rate = mean(failure_reason != "Success"),
    .groups = 'drop'
  ) %>%
  bind_rows(
    impossible_scenarios %>%
      select(panel_name, complexity_name) %>%
      distinct() %>%
      mutate(failure_rate = 1.0)
  ) %>%
  mutate(
    panel_name = factor(panel_name, levels = c("Wide Panel", "Square", "Long Panel")),
    complexity_name = factor(complexity_name, levels = c("Simple", "Standard", "Complex", "High-Dimensional"))
  )

figure1 <- ggplot(failure_rates_complete, 
                  aes(x = complexity_name, y = panel_name, fill = failure_rate)) +
  geom_tile(color = "white", lwd = 1.5) +
  geom_text(aes(label = scales::percent(failure_rate, accuracy = 0.1)), 
            color = "white", size = 4, fontface = "bold") +
  scale_y_discrete(labels = c(
    "Wide Panel" = "Wide Panel\n(N=400, T=4)",
    "Square" = "Square\n(N=200, T=8)",
    "Long Panel" = "Long Panel\n(N=100, T=16)"
  )) +
  scale_fill_gradient(
    low = "#1a9850",
    high = "#d73027",
    name = "Failure Rate",
    labels = scales::percent_format(),
    limits = c(0, 1)
  ) +
  labs(
    title = "Simulation Failure Rates by Panel Architecture and Model Complexity",
    x = "Model Complexity",
    y = "Panel Architecture"
  ) +
  theme_minimal(base_size = 12) +
  theme(
    legend.position = "right",
    plot.title = element_text(size = 14, face = "bold"),
    axis.text = element_text(face = "bold"),
    axis.text.y = element_text(hjust = 1)
  )
ggsave(file.path(results_dir, paste0("figure1_failure_rates_heatmap_", timestamp, ".png")), plot = figure1, 
       width = 10, height = 6, dpi = 300)

# FIGURE 2: Specificity by Model Complexity and Panel Architecture
if (nrow(test_data) > 50) {
  specificity_summary <- test_data %>%
    group_by(complexity_name, panel_name) %>%
    summarise(
      mean_specificity = mean(specificity, na.rm = TRUE),
      n_obs = n(),
      .groups = 'drop'
    ) %>%
    filter(n_obs >= 10) %>%
    mutate(
      panel_name = factor(panel_name, levels = c("Wide Panel", "Square", "Long Panel")),
      complexity_name = factor(complexity_name, levels = c("Simple", "Standard", "Complex", "High-Dimensional"))
    )
  multi_bar_data <- specificity_summary %>%
    group_by(complexity_name) %>%
    filter(n() > 2) %>%
    ungroup()
  two_bar_data <- specificity_summary %>%
    group_by(complexity_name) %>%
    filter(n() == 2) %>%
    ungroup()
  single_bar_data <- specificity_summary %>%
    group_by(complexity_name) %>%
    filter(n() == 1) %>%
    ungroup()
  figure2 <- ggplot(specificity_summary, aes(x = complexity_name, y = mean_specificity, 
                                             fill = panel_name, group = panel_name)) +
    geom_col(position = position_dodge2(width = 0.9, preserve = "single"), alpha = 0.8) +
    geom_label(
      data = multi_bar_data,
      aes(label = sprintf("%.3f", mean_specificity)),
      position = position_dodge2(width = 0.9),
      hjust = 0.6, vjust = -0.1,
      fill = NA, label.size = NA, size = 3.5
    ) +
    geom_label(
      data = two_bar_data,
      aes(label = sprintf("%.3f", mean_specificity)),
      position = position_dodge2(width = 0.6),
      hjust = 0.6, vjust = -0.1,
      fill = NA, label.size = NA, size = 3.5
    ) +
    geom_label(
      data = single_bar_data,
      aes(label = sprintf("%.3f", mean_specificity)),
      hjust = 0.5, vjust = -0.1,
      fill = NA, label.size = NA, size = 3.5
    ) +
    scale_y_continuous(limits = c(0, 1), labels = scales::percent_format()) +
    scale_fill_manual(
      values = c("Wide Panel" = "#2E86AB", "Square" = "#A23B72", "Long Panel" = "#F18F01"),
      na.value = "grey50"
    ) +
    labs(
      x = "Model Complexity",
      y = "Specificity",
      fill = "Panel Architecture"
    ) +
    theme_minimal(base_size = 12) +
    theme(
      axis.text.x = element_text(angle = 45, hjust = 1),
      legend.position = "bottom"
    )
  ggsave(file.path(results_dir, paste0("figure2_specificity_by_complexity_", timestamp, ".png")), plot = figure2, 
         width = 10, height = 6, dpi = 300)
}

# 11. FINAL SUMMARY REPORT
total_time <- as.numeric(difftime(Sys.time(), start_time, units = "mins"))
successful_runs <- sum(final_results_df$failure_reason == "Success", na.rm = TRUE)
summary_stats <- list(
  methodology = list(
    total_theoretical_scenarios = nrow(all_scenarios),
    excluded_impossible_scenarios = nrow(impossible_scenarios),
    simulated_feasible_scenarios = nrow(feasible_scenarios),
    total_simulation_runs = nrow(final_results_df),
    replications_per_scenario = N_REPLICATIONS
  ),
  results = list(
    total_successful_runs = successful_runs,
    success_rate = round(successful_runs / nrow(final_results_df) * 100, 1),
    viable_mechanisms = as.character(viable_mechanisms),
    runtime_minutes = round(total_time, 1)
  )
)
writeLines(
  c(
    "HAUSMAN TEST MONTE CARLO SIMULATION - FINAL SUMMARY",
    "METHODOLOGY:",
    sprintf("- Total theoretical scenarios: %d", summary_stats$methodology$total_theoretical_scenarios),
    sprintf("- Excluded impossible scenarios (T < k+1): %d", summary_stats$methodology$excluded_impossible_scenarios),
    sprintf("- Simulated feasible scenarios: %d", summary_stats$methodology$simulated_feasible_scenarios),
    sprintf("- Total simulation runs: %d", summary_stats$methodology$total_simulation_runs),
    sprintf("- Replications per scenario: %d", summary_stats$methodology$replications_per_scenario),
    "DESIGN FIX:",
    "- Equal sample sizes across all panel architectures (1,600 obs each)",
    "- Wide Panel: N=400, T=4",
    "- Square Panel: N=200, T=8",
    "- Long Panel: N=100, T=16",
    "RESULTS:",
    sprintf("- Successful runs: %d (%.1f%%)", summary_stats$results$total_successful_runs, 
            summary_stats$results$success_rate),
    sprintf("- Viable mechanisms: %s", paste(summary_stats$results$viable_mechanisms, collapse = ", ")),
    sprintf("- Runtime: %.1f minutes", summary_stats$results$runtime_minutes),
    "FILES GENERATED:",
    sprintf("- Raw data: %s", output_filename),
    sprintf("- Tables: table1_%s.csv through table8_%s.csv (as available)", timestamp, timestamp),
    sprintf("- Figures: figure1_failure_rates_heatmap_%s.png, figure2_specificity_by_complexity_%s.png", timestamp, timestamp),
    sprintf("- Analysis: failure_analysis_by_mechanism_%s.csv", timestamp),
    sprintf("- Documentation: excluded_impossible_scenarios_%s.csv", timestamp)
  ),
  file.path(results_dir, paste0("SIMULATION_SUMMARY_", timestamp, ".txt"))
)
